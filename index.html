<!DOCTYPE html>
<html lang="en">

  <head>
      <meta charset="utf-8" />
      <title>How the brain learns to learn</title>
      <meta http-equiv="content-type" content="text/html; charset=utf-8" />
      <style src="https://ehbeam.github.io/learning2learn/style.css"></style>
  </head>

  <body>

    <script src="https://ehbeam.github.io/learning2learn/main.js"></script>

    <center>
      
      <br/><br/><br/><h1>How the brain learns to learn</h1><br/>

      <div class="captionContainer" style="text-align:center">
        Review of Wang, J. X. <i>et al.</i> (2018) <a href="https://www.nature.com/articles/s41593-018-0147-8" target="_blank">Prefrontal cortex as a meta-reinforcement learning system</a>
      </div>

      <br/><br/><h4>By Ellie Beam</h4><br/><br/>

    </center>

    <div class="textContainer">
          <p>
             Artificial intelligence (AI) has rocketed ahead in its ability to recognize faces, drive cars, and make medical diagnoses. A remaining limitation is that AI requires hours or days of training to learn how to respond appropriately in each new situation. Humans, by contrast, can quickly "get" the crux of problems despite little direct experience. For a demonstration of your ability to do this, simply press play below: 
          </p>
    </div><br/><br/><br/>

    <canvas id="breakoutCanvas" width="420" height="380"></canvas>

    <center>
        <button id="playButton" onclick="draw();">Play</button>
        <button id="stopButton" onclick="stop();">Stop</button>
    </center><br/>

    <center><div class="captionContainer">
        <p style="text-align:center">
          Image adapted from <a href="https://cdn3.vectorstock.com/i/1000x1000/21/97/simple-line-drawn-vintage-game-arcade-cabinet-icon-vector-3632197.jpg" target="_blank">here</a>. Game adapted from <a href="https://github.com/end3r/Gamedev-Canvas-workshop" target="_blank">here</a>.
        </p>
    </div></center><br/><br/>

    <div class="textContainer">
        <p>
          Even if you've never played this game before, you were probably able to score pretty well on your first try. If you noticed the scoreboard, you likely sought out a strategy that would rack up points and prevent you from losing lives. This is an example of knowing how to learn in a new context by transferring over abstract rules from prior experiences. Humans instinctively "learn how to learn" in this way beginning in childhood.
        </p><br/>

        <p>
          Through deep learning, AI has exceeded human performance in the game above, called Breakout, and developed gaming strategies with an impressive level of sophistication.<a href="#footnote1" class="footnoteLink"><sup>1</sup></a> For example, AI has learned to win Breakout within just a few moves by bouncing the ball into the nook above the blocks where it can ricochet between them and the ceiling. Yet, AI has made little progress in learning how to apply general rules across games with a similar premise. A separate model has had to be trained afresh for each game in the Atari repertoire. 
        </p><br/>

        <p>
          In contrast to AI, the brain learns quickly when thrown into new situations. It's long been understood that the brain is able to track how large a reward it anticipates receiving from a given action. If the reward is greater than expected, then the action that led to it is reinforced by the release of the neurotransmitter dopamine from neurons in the midbrain. In addition to this basic "reinforcement learning" system, the prefrontal cortex also represents actions, expectations, and rewards in the activity of its neurons. Until recently, there was no overarching model to account for how the two systems work together to enable us to learn how to learn.
        </p>
    </div><br/><br/><br/>

    <center>

      <h2 id="titleRL" style="font-size: 22px;">What is reinforcement learning?</h2><br/>

      <span id="reverseRL" class="diagramButton" onclick="lastDiagram()"> << </span>

      <img id="diagramRL" width="580px" src="https://raw.githubusercontent.com/ehbeam/learning2learn/master/diagram/rl_1.png"/>

      <span id="forwardRL" class="diagramButton" onclick="nextDiagram()"> >> </span>

    </center><br/>

    <div class="captionContainer">
        
        <p id="captionRL">
          "Reinforcement learning" is a cycle of observing the world and taking actions that alter the world in order to maximize some reward. Observations are key to this cycle because they make it possible to learn which actions are most likely to yield a reward in a given set of circumstances. The agent in the reinforcement learning cycle may be a person, animal, or even a computer model.
        </p>

    </div><br/><br/><br/>

    <div class="textContainer">
        <p>
          At <a href="https://deepmind.com/" target="_blank">DeepMind</a>, researchers leverage knowledge of how the brain learns in order to develop more efficient AI models. Their director of neuroscience research, Matt Botvinick, has <a href="https://youtu.be/uv4Hh3wDH14" target="_blank">likened this strategy</a> to understanding how birds fly in order to build better planes. While there are plenty of differences between birds and planes, there are physical principles (e.g., lift) that may be gleaned from the natural implementation of flying and then applied to engineer powerful new technologies.
        </p><br/>

        <p>
          In a recent publication, DeepMind advanced this process of innovation one step further, showing that their new model "plane" (i.e., AI model) had achieved a property of natural "flight" (i.e., the brain's ability to learn how to learn).<a href="#footnote2" class="footnoteLink"><sup>2</sup></a> They started with the premise that the brain learns using the dopamine-based reinforcement system described above. One step further, Botvinick and his team theorized that dopamine neurons could set the synaptic weights in a second, meta-reinforcement learning system in the prefrontal cortex. They modeled this dual learning system using a recurrent neural network with simulated “dopamine” inputs.
        </p><br/>
    </div><br/><br/>

    <center><div id="brainContainer">
      <ul id="brainMap">

        <li><a class="vta" href="#" title="Ventral Tegmental Area"><span style="color:#366E6A"><b>Ventral Tegmental Area</b><br/>Contains the cell bodies of dopamine neurons in the mesocortical pathway (teal) and mesolimbic pathway (not shown).</span></a></li>

        <li><a class="mcpL" href="#" title="Mesocortical Pathway"><span style="color:#366E6A"><b>Mesocortical Pathway</b><br/>
        Dopamine neurons projecting from the ventral tegmental area to the prefrontal cortex.</span></a></li>

        <li><a class="mcpT" href="#" title="Mesocortical Pathway"><span style="color:#366E6A"><b>Mesocortical Pathway</b><br/>
        Dopamine neurons projecting from the ventral tegmental area to the prefrontal cortex.</span></a></li>

        <li><a class="mcpB" href="#" title="Mesocortical Pathway"><span style="color:#366E6A"><b>Mesocortical Pathway</b><br/>
        Dopamine neurons projecting from the ventral tegmental area to the prefrontal cortex.</span></a></li>

        <li><a class="sn" href="#" title="Substantia Nigra"><span style="color:#9D455F"><b>Substantia Nigra</b><br/>
        Contains the cell bodies of dopamine neurons in the nigrostriatal pathway. Its name translates to <i>black substance</i> in Latin, referring to the dark pigment released in the process of dopamine synthesis.</span></a></li>

        <li><a class="nsp" href="#" title="Nigrostriatal Pathway"><span style="color:#9D455F"><b>Nigrostriatal Pathway</b><br/>
        Dopamine neurons projecting from the substantia nigra to the caudate and putamen, which together make up the striatum.</span></a></li>

      </ul>
    </div></center><br/>

    <div class="captionContainer">
        <p>
          Midbrain dopamine neurons are classically thought to support reinforcement learning. The prefrontal cortex is thought to "learn how to learn" by implementing a meta-reinforcement learning algorithm. Its neural connections are weighted by input from dopamine neurons running in the <span class="pathwayLink" style="color:#366E6A">mesocortical pathway</span>. In addition to the role of dopamine in learning, there are dopamine neurons running in the <span class="pathwayLink" style="color:#9D455F">nigrostriatal pathway</span> which facilitate movement, and in the <span style="font-weight:bold">mesolimbic pathway</span> (not shown) which support emotional processing.
        </p>
    </div>
    <br/><br/><br/>

    <div class="textContainer">
        <p>
          The researchers showed that their model was able to learn like the brain by having it perform the same kinds of tasks as monkeys and humans in previous neuroscience studies. One of these tasks was adapted from a classic experiment led by Harry Harlow in 1949.<a href="#footnote3" class="footnoteLink"><sup>3</sup></a> In the original study, two objects were set before a monkey who would have to choose which one covered a food reward. The objects remained the same for six trials, over which the animal was able to learn which object was rewarded. Then, the objects were switched out for two new objects. You can play the same kind of task here:
        </p><br/>
    </div>

    <div id="harlowTask" width="800px" height="800px">

      <img id="airplane" class="harlowImg" src="https://raw.githubusercontent.com/ehbeam/learning2learn/master/harlow/images/airplane.png" />
      <img id="banana" class="harlowImg" src="https://raw.githubusercontent.com/ehbeam/learning2learn/master/harlow/images/banana.png" />
      <img id="camera" class="harlowImg" src="https://raw.githubusercontent.com/ehbeam/learning2learn/master/harlow/images/camera.png" />
      <img id="cat" class="harlowImg" src="https://raw.githubusercontent.com/ehbeam/learning2learn/master/harlow/images/cat.png" />
      <img id="cupcake" class="harlowImg" src="https://raw.githubusercontent.com/ehbeam/learning2learn/master/harlow/images/cupcake.png" />
      <img id="dog" class="harlowImg" src="https://raw.githubusercontent.com/ehbeam/learning2learn/master/harlow/images/dog.png" />
      <img id="donut" class="harlowImg" src="https://raw.githubusercontent.com/ehbeam/learning2learn/master/harlow/images/donut.png" />
      <img id="fox" class="harlowImg" src="https://raw.githubusercontent.com/ehbeam/learning2learn/master/harlow/images/fox.png" />
      <img id="icecream" class="harlowImg" src="https://raw.githubusercontent.com/ehbeam/learning2learn/master/harlow/images/icecream.png" />
      <img id="pizza" class="harlowImg" src="https://raw.githubusercontent.com/ehbeam/learning2learn/master/harlow/images/pizza.png" />
      <img id="shell" class="harlowImg" src="https://raw.githubusercontent.com/ehbeam/learning2learn/master/harlow/images/shell.png" />
      <img id="umbrella" class="harlowImg" src="https://raw.githubusercontent.com/ehbeam/learning2learn/master/harlow/images/umbrella.png" />

      <img id="BLANK" class="harlowImg" src="https://raw.githubusercontent.com/ehbeam/learning2learn/master/harlow/images/BLANK.png" />
      <img id="YES" class="harlowImg" src="https://raw.githubusercontent.com/ehbeam/learning2learn/master/harlow/images/YES.png" />
      <img id="NO" class="harlowImg" src="https://raw.githubusercontent.com/ehbeam/learning2learn/master/harlow/images/NO.png" /><br/>

      <center>
        <h2 id="instructions" style="font-size: 22px;">Choose an object:</h2>
        <canvas id="canvasA" class="canvas" width="300" height="300"></canvas>
        <canvas id="canvasB" class="canvas" width="300" height="300"></canvas>
      </center>

      <center>
        <span id="harlowCongrats"></span>
        <div id="harlowScore">
          <p>Score: <span id="score"></span></p>
        </div><br/>
      </center>

      <center>
        <div id="d3Plot"></div>
      </center>

    </div>

    <center><div class="captionContainer">
        <p>
          Wang <i>et al.</i> (2018) trained a meta-reinforcement learning model to perform the same kind of task as above,<a href="#footnote1" class="footnoteLink"><sup>1</sup></a> which was originally developed by the neurophysiologist Harry Harlow in 1949.<a href="#footnote3" class="footnoteLink"><sup>3</sup></a> Over six trials, one object in each pair is rewarded. The pair of objects is then switched. In order to demonstrate "learning to learn," the model proposed by Wang <i>et al.</i> would have to pick up on the rule of the game that one of the two objects in each new pair would be rewarded.
        </p>
    </div></center><br/><br/>

    <div class="textContainer">
        <p>
          When the brain performs this task, it initially chooses randomly between the objects. Over a few sets of objects, the brain is able to select the object hiding the reward after a single trial. This is an example of learning how to learn by applying an abstract rule from one scenario to another. Remarkably, the data recorded over the course of training Botvinick's meta-reinforcement learning model showed the same pattern as that recorded from behaving animals. In other words, the model could replicate the brain's process of learning how to learn.
        </p><br/>
    </div><br/><br/>

    <center>
      <img class="harlowPerf" width="630px" src="https://raw.githubusercontent.com/ehbeam/learning2learn/master/harlow/harlow_task.png"/>
    </center><br/>

    <center><div class="captionContainer">
        <p>
          Performance on the Harlow task in terms of accuracy, computed as the percentage of objects that were correctly selected. The overall pattern for improvement in accuracy over the course of learning was similar between the monkey (left) and the computer model (right). Adapted from Figure 6 of Wang <i>et al</i>.<a href="#footnote2" class="footnoteLink"><sup>2</sup></a>
        </p>
    </div></center><br/><br/>

    <div class="textContainer">
        <p>
          This line of research represents a fruitful intersection of computer science and neurobiology. However, if your interest is in understanding the brain, you may feel a tad dissatisfied. The researchers did not demonstrate that the brain necessarily implements a meta-reinforcement learning algorithm in its prefrontal cortex. Future researchers will need to offer evidence that dopamine neurons send signals to the prefrontal cortex that are indispensable for learning how to learn. One way to do this would be to disrupt dopamine inputs to the prefrontal cortex and show that animals can no longer improve across sets of trials in the Harlow task.
        </p><br/>

        <p>
          From the engineer's perspective, the results of the Botvinick study are of immediate interest in developing improved applications of AI. In the near future, more efficient reinforcement learning algorithms may facilitate dynamic and individualized medical diagnostics or accelerate the deployment of autonomous vehicles on the road. AI isn't just playing games anymore. It will be an exciting time for engineers and ethicists alike as technology advances up to and beyond the limits of our own neural mechanisms.
        </p><br/><br/>

        <h2>References</h2>

        <ol>

          <li id="footnote1">Mnih, V. <i>et al.</i> <a href="https://www.nature.com/articles/nature14236" target="_blank">Human-level control through deep reinforcement learning</a>. <i>Nature Neuroscience</i> (2015).</li>

          <li id="footnote2">Wang, J. X. <i>et al.</i> <a href="https://www.nature.com/articles/s41593-018-0147-8" target="_blank">Prefrontal cortex as a meta-reinforcement learning system</a>. <i>Nature Neuroscience</i> (2018).</li>

          <li id="footnote3">Harlow, H. <a href="https://psycnet.apa.org/record/1949-03097-001" target="_blank">The formation of learning sets</a>. <i>Psychological Review</i> (1949).</li>

        </ol><br/>
        
    </div><br/><br/><br/>

  </body>
</html>
