<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8" />
    <title>How the brain learns to learn</title>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
  
    <style>

        * { 
          font-family: Avenir;
          padding: 0px;
          margin: 0px;
        }

        body {
          width: 1000px;
          margin: auto;
        }

        h1 {
          font-size: 36px;
        }

        button {
          font-size: 14pt;
          color: white;
          background: #969696; 
          margin: 10px;
          padding: 5px;
          padding-left: 40px;
          padding-right: 40px;
          border-radius: 5px;
          border: black 2px solid;
          box-shadow: 3px 3px 5px #9b9b9b;
        }

        button:hover {
          cursor: pointer;
          background-color: #4f4f4f;
        }

        button:focus {
          outline: none;
        }

        a {
          color: #4c4c4c;
        }

        ol {
          padding-left: 50px;
        }

        li {
          margin: 10px 0;
        }

        .textContainer {
          display: inline-block;
          width: 800px;
          margin-left: 100px;
          margin-right: 100px;
        }

        .captionContainer {
          display: inline-block;
          width: 700px;
          margin-left: 150px;
          margin-right: 150px;
          text-align: center;
          font-size: 14px;
          color: gray;
        }

        #gameCanvas { 
          font-family: Avenir;
          background: #eee; 
          display: block; 
          margin: 0 auto;
          border-radius: 10px;
          border: black 2px solid;
          box-shadow: 3px 3px 5px #9b9b9b;
        }

        .taskPerf {
          padding: 12px;
          padding-right: 25px;
          border-radius: 10px;
          border: black 2px solid;
          box-shadow: 3px 3px 5px #9b9b9b;
        }

        #harlowTask {
          -webkit-touch-callout: none;
          -webkit-user-select: none;
          -khtml-user-select: none;
          -moz-user-select: none;
          -ms-user-select: none;
          user-select: none;
        }

        .canvas {
          border: 2px solid black; 
          border-radius: 10px;
          cursor: pointer;
          margin: 20px;
          background-color: white;
          box-shadow: 3px 3px 5px #9b9b9b;
        }

        .canvas:hover {
          box-shadow: 0px 0px 20px #d3c472;
        }

        .taskImg {
          display: none;
        }

        #scoreBoard {
          font-size: 20px;
          width: 660px; 
          height: 40px; 
          background: #e2e2e2; 
          border-radius: 25px;
          display: table-cell; 
          vertical-align: middle;
        }

        #congrats {
          font-size: 32px;
        }

        svg {
          display: block;
          margin: 0 auto;
        }

        .axis { 
          font: Avenir;
          font-size: 20px;
          stroke-width: 2px;
        }

        .line {
          fill: none;
          stroke: gray;
          stroke-width: 4px;
        }
          
        .overlay {
          fill: none;
          pointer-events: all;
        }

        .dot {
          fill: gray;
          stroke: gray;
        }
          
        .focus circle {
          fill: none;
          stroke: black;
        }

        #brainContainer {
          position: relative;
          display: inline-block;
          width: 800px;
        }

        #map {
          margin: 0px;
          padding: 0px;
          width: 600px;
          height: 400px;
          background: url(https://github.com/ehbeam/learning2learn/blob/master/dopamine_pathways.png?raw=true) top left no-repeat #fff;
          background-size: 500px;
          font-family: Avenir;
          font-size: 16px;
        }

        #map li {
          margin: 0;
          padding: 0;
          list-style: none;
        }

        #map li a {
          position: absolute;
          display: block;
          /*background: url(https://github.com/ehbeam/learning2learn/blob/master/images/BLANK.png?raw=true);*/
          text-decoration: none;
          color: #000;
        }

        #map li a span { 
          display: none; 
        }

        #map li a:hover span {
           position: absolute;
           display: block;
           width: 200px;
           left: 20px;
           top: 20px;
           border: 2px solid black;
           border-radius: 15px;
           background: white;
           padding: 15px;
           filter: alpha(opacity=80);
           opacity: 0.8;
        }

        #map a.vta {
          top: 214px;
          left: 338px;
          width: 23px;
          height: 25px;
        }

        #map a.mcpL {
          top: 30px;
          left: 120px;
          width: 100px;
          height: 200px;
        }

        #map a.mcpT {
          top: 15px;
          left: 220px;
          width: 115px;
          height: 100px;
        }

        #map a.mcpB {
          top: 180px;
          left: 175px;
          width: 120px;
          height: 90px;
        }
         
        #map a.sn {
          top: 198px;
          left: 352px;
          width: 24px;
          height: 28px;
        }

        #map a.nsp {
          top: 120px;
          left: 245px;
          width: 150px;
          height: 80px;
        }

    </style>
</head>

<body>

<script src="https://d3js.org/d3.v5.min.js"></script>

<br/><br/><br/>
<h1><center>How the brain learns to learn</center></h1>

<center>
  <div class="captionContainer"><br/>
  Review of Wang, J. X. <i>et al.</i> (2018) <a href="https://www.nature.com/articles/s41593-018-0147-8" target="_blank">Prefrontal cortex as a meta-reinforcement learning system</a>
  </div>
</center>
<br/>

<h4><center>By Ellie Beam</center></h4>
<br/>

<div class="textContainer">

    <p>
       Artificial intelligence (AI) has rocketed ahead in its ability to recognize faces, drive cars, and make medical diagnoses. A remaining limitation on the growth of AI is that it requires hours or days of training to learn how to respond appropriately in each new situation it encounters. Humans, by contrast, can quickly "get" the crux of problems despite little direct experience. As a quick demonstration of this, press play below.
    </p>
</div>
<br/><br/>

<br/>
<canvas id="gameCanvas" width="420" height="380"></canvas>

<script>
    var canvas = document.getElementById("gameCanvas");
    var ctx = canvas.getContext("2d");
    var ballRadius = 10;
    var x = canvas.width/2;
    var y = canvas.height-30;
    var dx = 2;
    var dy = -2;
    var paddleHeight = 10;
    var paddleWidth = 75;
    var paddleX = (canvas.width-paddleWidth)/2;
    var rightPressed = false;
    var leftPressed = false;
    var brickRowCount = 5;
    var brickColumnCount = 3;
    var brickWidth = 70;
    var brickHeight = 20;
    var brickPadding = 9;
    var brickOffsetTop = 40;
    var brickOffsetLeft = 16;
    var score = 0;
    var lives = 3;

    var bricks = [];
    for(var c=0; c<brickColumnCount; c++) {
        bricks[c] = [];
        for(var r=0; r<brickRowCount; r++) {
            bricks[c][r] = { x: 0, y: 0, status: 1 };
        }
    }

    document.addEventListener("keydown", keyDownHandler, false);
    document.addEventListener("keyup", keyUpHandler, false);
    document.addEventListener("mousemove", mouseMoveHandler, false);

    function keyDownHandler(e) {
        if(e.keyCode == 39) {
            rightPressed = true;
        }
        else if(e.keyCode == 37) {
            leftPressed = true;
        }
    }
    function keyUpHandler(e) {
        if(e.keyCode == 39) {
            rightPressed = false;
        }
        else if(e.keyCode == 37) {
            leftPressed = false;
        }
    }
    function mouseMoveHandler(e) {
        var relativeX = e.clientX - canvas.offsetLeft;
        if(relativeX > 0 && relativeX < canvas.width) {
            paddleX = relativeX - paddleWidth/2;
        }
    }
    function collisionDetection() {
        for(var c=0; c<brickColumnCount; c++) {
            for(var r=0; r<brickRowCount; r++) {
                var b = bricks[c][r];
                if(b.status == 1) {
                    if(x > b.x && x < b.x+brickWidth && y > b.y && y < b.y+brickHeight) {
                        dy = -dy;
                        b.status = 0;
                        score++;
                        if(score == brickRowCount*brickColumnCount) {
                            alert("You win!");
                            document.location.reload();
                        }
                    }
                }
            }
        }
    }

    function drawBall() {
        ctx.beginPath();
        ctx.arc(x, y, ballRadius, 0, Math.PI*2);
        ctx.fillStyle = "#366E6A";
        ctx.fill();
        ctx.closePath();
    }
    function drawPaddle() {
        ctx.beginPath();
        ctx.rect(paddleX, canvas.height-paddleHeight, paddleWidth, paddleHeight);
        ctx.fillStyle = "#606060";
        ctx.fill();
        ctx.closePath();
    }
    function drawBricks() {
        for(var c=0; c<brickColumnCount; c++) {
            for(var r=0; r<brickRowCount; r++) {
                if(bricks[c][r].status == 1) {
                    var brickX = (r*(brickWidth+brickPadding))+brickOffsetLeft;
                    var brickY = (c*(brickHeight+brickPadding))+brickOffsetTop;
                    bricks[c][r].x = brickX;
                    bricks[c][r].y = brickY;
                    ctx.beginPath();
                    ctx.rect(brickX, brickY, brickWidth, brickHeight);
                    ctx.fillStyle = "#9D455F";
                    ctx.fill();
                    ctx.closePath();
                }
            }
        }
    }
    function drawScore() {
        ctx.font = "16px Avenir";
        ctx.fillStyle = "gray";
        ctx.fillText("Score: "+score, 15, 25);
    }
    function drawLives() {
        ctx.font = "16px Avenir";
        ctx.fillStyle = "gray";
        ctx.fillText("Lives: "+lives, canvas.width-70, 25);
    }
    function drawPause() {
      var img = new Image();
      img.src = "https://raw.githubusercontent.com/ehbeam/learning2learn/master/arcade_game_tp.png";
      // Original source is "https://image.flaticon.com/icons/svg/744/744594.svg"
      img.onload = function(){
        ctx.drawImage(img, 20, 5, 380, 370);
      }
    }

    function draw() {
        ctx.clearRect(0, 0, canvas.width, canvas.height);
        drawBricks();
        drawBall();
        drawPaddle();
        drawScore();
        drawLives();
        collisionDetection();

        if(x + dx > canvas.width-ballRadius || x + dx < ballRadius) {
            dx = -dx;
        }
        if(y + dy < ballRadius) {
            dy = -dy;
        }
        else if(y + dy > canvas.height-ballRadius) {
            if(x > paddleX && x < paddleX + paddleWidth) {
                dy = -dy;
            }
            else {
                lives--;
                if(!lives) {
                    alert("Game over");
                    document.location.reload();
                }
                else {
                    x = canvas.width/2;
                    y = canvas.height-30;
                    dx = 3;
                    dy = -3;
                    paddleX = (canvas.width-paddleWidth)/2;
                }
            }
        }

        if(rightPressed && paddleX < canvas.width-paddleWidth) {
            paddleX += 7;
        }
        else if(leftPressed && paddleX > 0) {
            paddleX -= 7;
        }

        x += dx;
        y += dy;
        requestAnimationFrame(draw);
    }

    function stop() {
        document.location.reload();
        drawPause();
    }

    ctx.clearRect(0, 0, canvas.width, canvas.height);
    drawPause();

</script>

<center>
    <button id="playButton" onclick="draw();">Play</button>
    <button id="stopButton" onclick="stop();">Stop</button>
</center>

<center><div class="captionContainer">
    <p>
      Image adapted from <a href="https://cdn3.vectorstock.com/i/1000x1000/21/97/simple-line-drawn-vintage-game-arcade-cabinet-icon-vector-3632197.jpg" target="_blank">here</a>. Game adapted from <a href="https://github.com/end3r/Gamedev-Canvas-workshop" target="_blank">here</a>.
    </p>
</div></center>
<br/><br/>

<div class="textContainer">
    <p>
      Even if you've never played Breakout before, you were probably able to score pretty well on your first try. If you noticed the scoreboard, you may have gone into the first few rounds of play seeking out a strategy that would rack up points and prevent you from losing lives. This is an example of knowing how to learn in a new context by transferring over abstract rules from prior related experiences. Humans instinctively "learn how to learn" in this way beginning in early childhood. 
    </p><br/>

    <p>
      Through deep learning, AI has exceeded human performance in Breakout and developed gaming strategies with an impressive level of sophistication (e.g., bouncing the ball into the nook above the blocks where it can ricochet between them and the ceiling).<sup>1</sup> Yet, AI has made little progress in learning how to apply general rules across games with a similar premise. For each game in the Atari repertoire, a separate AI model has had to learn the rules from scratch. Each of these models has required dozens or hundreds of trials to appreciate the basic objective of its corresponding game, let alone reach competitive levels of play. 
    </p><br/>

    <p>
      In contrast to AI, the brain learns quickly when thrown into a new situation. It's long been understood that the brain is able to track how big of a reward it anticipates receiving from a given action. If the reward is larger than expected, then the action that led to it is reinforced by the release of the neurotransmitter dopamine from neurons in subcortical regions. In addition to this first system for learning, the prefrontal cortex also represents rewards, actions, and expectations in the activity of its neurons. Until recently, there was no overarching model for how the subcortical and prefrontal systems work together to support the ability to learn how to learn.
    </p>
</div>
<br/><br/><br/>

<center><div id="brainContainer">
  <ul id="map">

    <li><a class="vta" href="#" title="Ventral Tegmental Area"><span style="color:#366E6A"><b>Ventral Tegmental Area</b><br/>Contains the cell bodies of dopamine neurons in the mesocortical pathway (teal) and mesolimbic pathway (not shown).</span></a></li>

    <li><a class="mcpL" href="#" title="Mesocortical Pathway"><span style="color:#366E6A"><b>Mesocortical Pathway</b><br/>
    Dopamine neurons projecting from the ventral tegmental area to the prefrontal cortex.</span></a></li>

    <li><a class="mcpT" href="#" title="Mesocortical Pathway"><span style="color:#366E6A"><b>Mesocortical Pathway</b><br/>
    Dopamine neurons projecting from the ventral tegmental area to the prefrontal cortex.</span></a></li>

    <li><a class="mcpB" href="#" title="Mesocortical Pathway"><span style="color:#366E6A"><b>Mesocortical Pathway</b><br/>
    Dopamine neurons projecting from the ventral tegmental area to the prefrontal cortex.</span></a></li>

    <li><a class="sn" href="#" title="Substantia Nigra"><span style="color:#9D455F"><b>Substantia Nigra</b><br/>
    Contains the cell bodies of dopamine neurons in the nigrostriatal pathway. Its name translates to <i>black substance</i> in Latin, referring to the dark pigment released in the process of dopamine synthesis.</span></a></li>

    <li><a class="nsp" href="#" title="Nigrostriatal Pathway"><span style="color:#9D455F"><b>Nigrostriatal Pathway</b><br/>
    Dopamine neurons projecting from the substantia nigra to the caudate and putamen, which together make up the striatum.</span></a></li>

  </ul>
</div></center>

<center><div class="captionContainer">
    <p>
      The prefrontal cortex (teal) is thought to "learn how to learn" by implementing a meta-reinforcement learning algorithm. Its synaptic weights are tuned by input from midbrain dopamine neurons, which are classically thought to support reinforcement learning. Hover over different parts of the brain to learn more about the anatomy of its dopamine pathways.
    </p>
</div></center>
<br/><br/>

<div class="textContainer">
    <p>
       At <a href="https://deepmind.com/" target="_blank">DeepMind</a>, researchers leverage knowledge of how the brain learns in order to develop more efficient AI algorithms. Their director of neuroscience research, Matt Botvinick, has <a href="https://youtu.be/uv4Hh3wDH14" target="_blank">likened this strategy</a> to understanding how birds fly in order to build better planes. While there are plenty of differences between birds and planes, there are physical principles (e.g., lift) that may be gleaned from the natural implementation of flying and then applied to engineer new, more powerful technologies.
    </p><br/>

    <p>
       In a recent publication, DeepMind advanced this process of innovation one step further, showing that their new model "plane" (i.e., AI algorithm) had achieved a property of natural "flight" (i.e., the brain's ability to learn how to learn).<sup>2</sup> They started with the premise that the brain typically learns by representing the difference between expectations and outcomes using dopamine as described above. One step further, Botvinick and his team theorized that dopamine neurons could set the synaptic weights in a second, <i>meta</i>-reinforcement learning system in the prefrontal cortex (shown in red below). They modeled this dual learning system using a recurrent neural network that simulated dopamine signals and a separate prefrontal learning system.
    </p><br/>

    <p>
      The researchers went on to show that their model was able to learn like the brain by having it perform the same kinds of tasks as monkeys and humans in previous neuroscience studies. One of these tasks was adapted from a classic experiment led by Harry Harlow in 1949.<sup>3</sup> In the original study, two objects were set before a monkey who would have to choose which one covered a food reward. The objects remained the same for six trials, over which the animal was able to learn which object was rewarded. Then, the objects were switched out for two new objects. The meta-reinforcement learning model performed a digital version of this task in which the objects were displayed on simulated screens. You can play the same kind of task below.
    </p><br/>
</div>

<div id="harlowTask" width="800px" height="800px">

  <img id="airplane" class="taskImg" src="https://raw.githubusercontent.com/ehbeam/learning2learn/master/images/airplane.png" />
  <img id="banana" class="taskImg" src="https://raw.githubusercontent.com/ehbeam/learning2learn/master/images/banana.png" />
  <img id="camera" class="taskImg" src="https://raw.githubusercontent.com/ehbeam/learning2learn/master/images/camera.png" />
  <img id="cat" class="taskImg" src="https://raw.githubusercontent.com/ehbeam/learning2learn/master/images/cat.png" />
  <img id="cupcake" class="taskImg" src="https://raw.githubusercontent.com/ehbeam/learning2learn/master/images/cupcake.png" />
  <img id="dog" class="taskImg" src="https://raw.githubusercontent.com/ehbeam/learning2learn/master/images/dog.png" />
  <img id="donut" class="taskImg" src="https://raw.githubusercontent.com/ehbeam/learning2learn/master/images/donut.png" />
  <img id="fox" class="taskImg" src="https://raw.githubusercontent.com/ehbeam/learning2learn/master/images/fox.png" />
  <img id="icecream" class="taskImg" src="https://raw.githubusercontent.com/ehbeam/learning2learn/master/images/icecream.png" />
  <img id="pizza" class="taskImg" src="https://raw.githubusercontent.com/ehbeam/learning2learn/master/images/pizza.png" />
  <img id="shell" class="taskImg" src="https://raw.githubusercontent.com/ehbeam/learning2learn/master/images/shell.png" />
  <img id="umbrella" class="taskImg" src="https://raw.githubusercontent.com/ehbeam/learning2learn/master/images/umbrella.png" />

  <img id="BLANK" class="taskImg" src="https://raw.githubusercontent.com/ehbeam/learning2learn/master/images/BLANK.png" />
  <img id="YES" class="taskImg" src="https://raw.githubusercontent.com/ehbeam/learning2learn/master/images/YES.png" />
  <img id="NO" class="taskImg" src="https://raw.githubusercontent.com/ehbeam/learning2learn/master/images/NO.png" />

  <br/>

  <center>
    <h2 id="instructions" style="font-size: 22px;">Choose an object:</h2>
    <canvas id="canvasA" class="canvas" width="300" height="300"></canvas>
    <canvas id="canvasB" class="canvas" width="300" height="300"></canvas>
  </center>

  <center>
    <span id="congrats"></span>
    <div id="scoreBoard">
      <p>Score: <span id="score"></span></p>
    </div>
    <br/>
  </center>

  <center>
    <div id="d3Plot"></div>
  </center>

</div>

<script type="text/javascript">

  var plotLine = function(data) {
    var margin = {top: 50, right: 50, bottom: 100, left: 100}
      , width = 800 - margin.left - margin.right 
      , height = 350 - margin.top - margin.bottom;
    var n = data.length;
    var xScale = d3.scaleLinear()
        .domain([-0.5, n-0.5])
        .range([0, width]);
    var yScale = d3.scaleLinear()
        .domain([0, 6]) 
        .range([height, 0]); 
    var line = d3.line()
        .x(function(d, i) { return xScale(i); }) 
        .y(function(d) { return yScale(d.y); }) 
        .curve(d3.curveMonotoneX) 
    var dataset = d3.range(n).map(function(d) { return {"y": data[d] } })
    var svg = d3.select("#d3Plot").append("svg")
        .attr("width", width + margin.left + margin.right)
        .attr("height", height + margin.top + margin.bottom)
      .append("g")
        .attr("transform", "translate(" + margin.left + "," + margin.top + ")");
    svg.append("g")
        .attr("class", "x axis")
        .attr("transform", "translate(0," + height + ")")
        .call(d3.axisBottom(xScale)
          .ticks(n)); 
    svg.append("g")
        .attr("class", "y axis")
        .call(d3.axisLeft(yScale)
          .ticks(6)); 
    svg.append("path")
        .datum(dataset) 
        .attr("class", "line") 
        .attr("d", line); 
    svg.selectAll(".dot")
        .data(dataset)
      .enter().append("circle") 
        .attr("class", "dot") 
        .attr("cx", function(d, i) { return xScale(i) })
        .attr("cy", function(d) { return yScale(d.y) })
        .attr("r", 6)
      svg.append("text")             
        .attr("transform",
              "translate(" + (width / 2) + " ," + 
                             (height + margin.top + 8) + ")")
        .style("text-anchor", "middle")
        .style("font-size", "24px")
        .text("Epoch");
      svg.append("text")
        .attr("transform", "rotate(-90)")
        .attr("y", 0 - margin.left + 30)
        .attr("x", 0 - (height / 2))
        .attr("dy", "1em")
        .style("text-anchor", "middle")
        .style("font-size", "24px")
        .text("Score");   
  };

  var endGame = function(scores) {
    document.getElementById("instructions").style.display = "none";
    document.getElementById("canvasA").style.display = "none";
    document.getElementById("canvasB").style.display = "none";
    document.getElementById("congrats").innerHTML = "You finished!<br/><br style='line-height:5px;'/>";
    document.getElementById("scoreBoard").innerHTML = "Scores: " + scores.join(", ");
  };

  var imagesA = ["cat", "airplane", "shell", "dog", "donut", "icecream"];
  var imagesB = ["cupcake", "fox", "banana", "camera", "pizza", "umbrella"];

  var correct = ["A", "B", "B", "A", "A", "B"]

  var nTrials = 0; var nEpochs = 0; var nPoints = 0; var scores = [];
  var dx = 20; var dy = 20; var width = 250; var height = 250;

  document.getElementById("score").innerHTML = nPoints;

  var cA = document.getElementById("canvasA");
  var ctxA = cA.getContext("2d");
  var imgA = new Image();
  imgA.src = "https://raw.githubusercontent.com/ehbeam/learning2learn/master/images/" + imagesA[0] + ".png";
  imgA.onload = function(){ ctxA.drawImage(imgA, dx, dy, width, height) };

  var cB = document.getElementById("canvasB");
  var ctxB = cB.getContext("2d");
  var imgB = new Image();
  imgB.src = "https://raw.githubusercontent.com/ehbeam/learning2learn/master/images/" + imagesB[0] + ".png";
  imgB.onload = function(){ ctxB.drawImage(imgB, dx, dy, width, height) };

  var toggle = "OFF"
  document.getElementById("canvasA").addEventListener("click", function(){
    if (nEpochs < 6) {
      if (nTrials < 6) {
        if (toggle == "OFF") {
          toggle = "ON";
          nTrials = nTrials + 1;
          if (correct[nEpochs] == "A") {
            ctxA.globalAlpha = 0.65;
            var imgA = document.getElementById("YES");
            ctxA.drawImage(imgA, 0, 0, width+50, height+50);
            ctxA.globalAlpha = 1.0;
            nPoints = nPoints + 1;
            document.getElementById("score").innerHTML = nPoints;
          } else {
            ctxA.globalAlpha = 0.65;
            var imgA = document.getElementById("NO");
            ctxA.drawImage(imgA, 0, 0, width+50, height+50);
            ctxA.globalAlpha = 1.0;
            document.getElementById("score").innerHTML = nPoints;
        }
      } else {
        toggle = "OFF";
        var imgBlank = document.getElementById("BLANK");
        ctxA.drawImage(imgBlank, dx, dy, width, height);
        var imgA = document.getElementById(imagesA[nEpochs]);
        ctxA.drawImage(imgA, dx, dy, width, height);
        ctxB.drawImage(imgBlank, dx, dy, width, height);
        var imgB = document.getElementById(imagesB[nEpochs]);
        ctxB.drawImage(imgB, dx, dy, width, height);
        document.getElementById("score").innerHTML = nPoints;
      } 
    } else {
        nTrials = 0; 
        nEpochs = nEpochs + 1;
        scores.push(nPoints);
        nPoints = 0;
      }
  } else {
      endGame(scores);
      plotLine(scores);
    }
  });

  document.getElementById("canvasB").addEventListener("click", function(){
    if (nEpochs < 6) {
      if (nTrials < 6) {
        if (toggle == "OFF") {
          toggle = "ON";
          nTrials = nTrials + 1;
          if (correct[nEpochs] == "B") {
            ctxB.globalAlpha = 0.65;
            var imgB = document.getElementById("YES");
            ctxB.drawImage(imgB, 0, 0, width+50, height+50);
            ctxB.globalAlpha = 1.0;
            nPoints = nPoints + 1;
            document.getElementById("score").innerHTML = nPoints;
          } else {
            ctxB.globalAlpha = 0.65;
            var imgB = document.getElementById("NO");
            ctxB.drawImage(imgB, 0, 0, width+50, height+50);
            ctxB.globalAlpha = 1.0;
            document.getElementById("score").innerHTML = nPoints;
          }
        } else {
          toggle = "OFF";
          var imgBlank = document.getElementById("BLANK");
          ctxA.drawImage(imgBlank, dx, dy, width, height);
          var imgA = document.getElementById(imagesA[nEpochs]);
          ctxA.drawImage(imgA, dx, dy, width, height);
          ctxB.drawImage(imgBlank, dx, dy, width, height);
          var imgB = document.getElementById(imagesB[nEpochs]);
          ctxB.drawImage(imgB, dx, dy, width, height);
          document.getElementById("score").innerHTML = nPoints;
        } 
      } else {
      nTrials = 0; 
        nEpochs = nEpochs + 1;
        scores.push(nPoints);
        nPoints = 0;
    }
    } else {
      endGame(scores);
      plotLine(scores);
    } 
  });

</script>

<br/>
<center><div class="captionContainer">
    <p>
      Wang <i>et al.</i> (2018) trained a meta-reinforcement learning model to perform the same kind of task as above,<sup>1</sup> which was originally developed by the neurophysiologist Harry Harlow in 1949.<sup>3</sup> Over six trials, one object in each pair is rewarded. The pair of objects is then switched. In order to demonstrate "learning to learn," the model would have to pick up on the rule of the game that one of the two objects in each new pair would be rewarded.
    </p>
</div></center>
<br/><br/>

<div class="textContainer">
    <p>
      When the brain performs this task, it initially chooses randomly between the objects. Over a few sets of objects, the brain is able to select the object hiding the reward after a single trial. This is an example of learning how to learn by applying an abstract rule from one scenario to another. Remarkably, the data recorded over the course of training Botvinick's meta-reinforcement learning model showed the same pattern as that recorded from behaving animals. In other words, the model could replicate the brain's process of learning how to learn. 
    </p><br/>
</div><br/><br/>

<center>
  <img class="taskPerf" width="630px" src="https://raw.githubusercontent.com/ehbeam/learning2learn/master/harlow_task.png"/>
</center>
<br/>

<center><div class="captionContainer">
    <p>
      Performance on the Harlow task. The overall pattern and number of object sets require to reach peak performance was similar between the monkey (left) and the meta-reinforcement learning model (right). Adapted from Figure 6 of Wang <i>et al</i>.<sup>2</sup>
    </p>
</div></center>
<br/><br/>

<div class="textContainer">
    <p>
      This line of research represents a fruitful intersection of computer science and neurobiology that has already borne promising results. However, if your primary interest is in understanding the brain, you may be left a tad dissatisfied. The researchers did <i>not</i> demonstrate here that the brain necessarily implements a meta-reinforcement learning algorithm in its prefrontal cortex. Future researchers will need to offer evidence that dopamine neurons send signals to the prefrontal cortex that are indispensable for learning how to learn. One way to do this would be to show that when dopamine signals to the prefrontal cortex are disrupted, animals can no longer improve across sets of trials in the Harlow task.
    </p><br/>

    <p>
      From the engineer's perspective, the results of the Botvinick study are of immediate interest in developing improved applications of AI. In the near future, more efficient reinforcement learning algorithms may facilitate dynamic and individualized medical diagnostics; they may accelerate the deployment of autonomous vehicles on the road. AI isn't just playing games anymore. It will be an exciting time for engineers and ethicists alike as technology advances up to and beyond the limits of our own neural mechanisms.
    </p><br/><br/>

    <h2>
      References
    </h2>

    <ol>

      <li>Mnih, V. <i>et al.</i> <a href="https://www.nature.com/articles/s41593-018-0147-8" target="_blank">Playing Atari with deep reinforcement learning</a>. Preprint in <i>arXiv</i> (2013).</li>

      <li>Wang, J. X. <i>et al.</i> <a href="https://www.nature.com/articles/s41593-018-0147-8" target="_blank">Prefrontal cortex as a meta-reinforcement learning system</a>. <i>Nature Neuroscience</i> (2018).</li>

      <li>Harlow, H. <a href="https://psycnet.apa.org/record/1949-03097-001" target="_blank">The formation of learning sets</a>. <i>Psychological Review</i> (1949).</li>

    </ol><br/>
    
</div>
<br />

</body>
</html>
